{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CXR inference TF-TRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNet201로 학습된 모델의 Inference 성능 향상을 위해 TensroRT를 사용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from Seg_modules import *\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check TensorFlow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-65996cac02ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check TensorRT version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT version: \n",
      "ii  libnvinfer-bin              7.2.1-1+cuda11.1                    amd64        TensorRT binaries\n",
      "ii  libnvinfer-dev              7.2.1-1+cuda11.1                    amd64        TensorRT development libraries and headers\n",
      "ii  libnvinfer-plugin-dev       7.2.1-1+cuda11.1                    amd64        TensorRT plugin libraries\n",
      "ii  libnvinfer-plugin7          7.2.1-1+cuda11.1                    amd64        TensorRT plugin libraries\n",
      "ii  libnvinfer7                 7.2.1-1+cuda11.1                    amd64        TensorRT runtime libraries\n",
      "ii  python3-libnvinfer          7.2.1-1+cuda11.1                    amd64        Python 3 bindings for TensorRT\n",
      "ii  python3-libnvinfer-dev      7.2.1-1+cuda11.1                    amd64        Python 3 development package for TensorRT\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorRT version: \")\n",
    "!dpkg -l | grep nvinfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "seg_model = load_model('../models/image_models/seg_model.h5',custom_objects={'dice_coef_loss': dice_coef_loss,'dice_coef':dice_coef})\n",
    "DenseNet = load_model('../models/image_models/DenseNet201.h5')\n",
    "img_model = load_model('../models/image_models/Covid_2class.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image path\n",
    "\n",
    "# COVID\n",
    "image_path=\"./test_data/covid-1.png\"\n",
    "\n",
    "# NON-COVID\n",
    "# image_path = \"./test_data/normal-1.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=(224,224)\n",
    "label_list = ('negative','positive')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# LOAD image and data preprocessing\n",
    "cropped_image = get_cropped_image(image_path, seg_model)\n",
    "\n",
    "img = tf.keras.preprocessing.image.array_to_img(cropped_image)\n",
    "img = img.resize(img_size)\n",
    "img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img = img / 255.0\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "feature_vector = DenseNet.predict(img)\n",
    "\n",
    "prediction = img_model.predict(feature_vector)[0]\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"경과 시간: \", elapsed_time)\n",
    "\n",
    "print(\"prediction: \", prediction)\n",
    "\n",
    "val = prediction.item(0)\n",
    "print(\"val: \", val)\n",
    "\n",
    "idx = int(np.round(val))\n",
    "print(\"idx: \", idx)\n",
    "\n",
    "label = label_list[idx]\n",
    "print(\"label: \", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 Inference Code\n",
    "\n",
    "### TF-TRT FP16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_FP16_model(input_saved_model_dir, output_saved_model_dir):\n",
    "    print('Converting to TF-TRT FP16...')\n",
    "\n",
    "    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
    "        precision_mode=trt.TrtPrecisionMode.FP16,\n",
    "        max_workspace_size_bytes=8000000000)\n",
    "\n",
    "    converter = trt.TrtGraphConverterV2(\n",
    "       input_saved_model_dir=input_saved_model_dir, conversion_params=conversion_params)\n",
    "\n",
    "    converter.convert()\n",
    "\n",
    "    converter.save(output_saved_model_dir=output_saved_model_dir)\n",
    "\n",
    "    print('Done Converting to TF-TRT FP16')\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_FP16_model(\"../models/image_models/DenseNet201\", \"./model/CXR_DenseNet_FP16_saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_FP16_model(\"../models/image_models/Covid_2class\", \"./model/CXR_C2C_FP16_saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_FP16_model(\"../models/image_models/seg_model\", \"./model/CXR_seg_FP16_saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = load_model('../models/image_models/seg_model.h5',custom_objects={'dice_coef_loss': dice_coef_loss,'dice_coef':dice_coef})\n",
    "DenseNet = load_model('./model/CXR_DenseNet_FP16_saved_model')\n",
    "img_model = load_model('./model/CXR_C2C_FP16_saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE inference Function\n",
    "def predict_cxr(image_path):\n",
    "    img_size = (224, 224)\n",
    "    label_list = ('negative','positive')\n",
    "    cropped_image = get_cropped_image(image_path, seg_model)\n",
    "    img = tf.keras.preprocessing.image.array_to_img(cropped_image)\n",
    "    img = img.resize(img_size)\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = img / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    cropped_img = tf.constant(img)\n",
    "        \n",
    "    signature_keys = list(DenseNet.signatures.keys())\n",
    "    infer = DenseNet.signatures[signature_keys[0]]\n",
    "    \n",
    "    pred = infer(cropped_img)\n",
    "\n",
    "    key = list(pred.keys())[0]\n",
    "    val = pred[key]\n",
    "    \n",
    "    signature_keys = list(img_model.signatures.keys())\n",
    "    inference = img_model.signatures[signature_keys[0]]\n",
    "    \n",
    "    prediction = inference(val)\n",
    "    \n",
    "    key = list(prediction.keys())[0]\n",
    "    value = prediction[key].numpy()[0]\n",
    "    \n",
    "    idx = int(np.round(value))\n",
    "    label = label_list[idx]\n",
    "        \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "infer = predict_cxr(image_path)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"경과 시간: \", end_time - start_time)\n",
    "print(\"Inference: \", infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
